{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()  # Output pixel values between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_images_to_tensors(path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    files = os.listdir(path)\n",
    "    images = []\n",
    "    for i in files:\n",
    "        img_tensor = transform((Image.open(path + i)).convert('L'))\n",
    "        images.append(img_tensor)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sketches_to_tensors(path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    files = os.listdir(path)\n",
    "    images = []\n",
    "    for i in files:\n",
    "        img_tensor = transform(Image.open(path + i))\n",
    "        images.append(img_tensor)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image dataset and their corresponding sketches\n",
    "image_dataset = transform_images_to_tensors('E:/ANN_PROJECT/ALL_Images/')\n",
    "\n",
    "for i in range(0, 10000, 32):\n",
    "    smaller_tensor1 = image_dataset[i : i+32]\n",
    "    torch.stack(smaller_tensor1)\n",
    "    filename1 = \"ImageTensor\"+str(i//32)+\".pt\"\n",
    "    torch.save(smaller_tensor1, \"E:/ANN_PROJECT/ImageTensors/\" + filename1)\n",
    "\n",
    "del image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_dataset = transform_sketches_to_tensors('E:/ANN_PROJECT/EqualizedSketches/')\n",
    "\n",
    "for i in range(0, 10000, 32):\n",
    "    smaller_tensor2 = sketch_dataset[i : i+32]\n",
    "    torch.stack(smaller_tensor2)\n",
    "    filename2 = \"SketchTensor\"+str(i//32)+\".pt\"\n",
    "    torch.save(smaller_tensor2, \"E:/ANN_PROJECT/SketchTensors/\" + filename2)\n",
    "\n",
    "del sketch_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3 , weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_dir = \"E:/ANN_PROJECT/ImageTensors/\"\n",
    "y_data_dir = \"E:/ANN_PROJECT/SketchTensors/\"\n",
    "num_epochs = 30\n",
    "\n",
    "x_data_files = [f for f in os.listdir(x_data_dir) if f.endswith(\".pt\")]\n",
    "y_data_files = [f for f in os.listdir(y_data_dir) if f.endswith(\".pt\")]\n",
    "# loop through each x_train file for 30 epochs\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss = 0\n",
    "    for j in range(len(x_data_files) - 1):\n",
    "        x_data = torch.load(os.path.join(x_data_dir, x_data_files[j]))\n",
    "        x_data = torch.stack(x_data)\n",
    "\n",
    "        y_data = torch.load(os.path.join(y_data_dir, y_data_files[j]))\n",
    "        y_data = torch.stack(y_data)\n",
    "\n",
    "        data = TensorDataset(x_data, y_data)\n",
    "        data_loader = DataLoader(data, batch_size=32, shuffle=True)\n",
    "\n",
    "        \n",
    "        for i,data in enumerate(data_loader):\n",
    "            img, sketch = data\n",
    "\n",
    "            img = img.to('cuda')\n",
    "            sketch = sketch.to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(img)\n",
    "            loss = criterion(output, sketch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "\n",
    "        del x_data\n",
    "        del y_data\n",
    "\n",
    "\n",
    "    # print training progress after each epoch\n",
    "    print(\"EPOCH : \" , epoch+1 , \"LOSS : \" , running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model architecture\n",
    "model = Autoencoder()  # Replace with your model architecture\n",
    "\n",
    "model_path = 'model.pth'\n",
    "model = torch.load(model_path , map_location=torch.device('cpu'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_dir = \"E:/ANN_PROJECT/ImageTensors/\"\n",
    "y_data_dir = \"E:/ANN_PROJECT/SketchTensors/\"\n",
    "\n",
    "x_data_files = [f for f in os.listdir(x_data_dir) if f.endswith(\".pt\")]\n",
    "\n",
    "x_data = torch.load(os.path.join(x_data_dir , x_data_files[-1]))\n",
    "\n",
    "x_data = torch.stack(x_data)\n",
    "\n",
    "# print(x_data.shape)\n",
    "# print(len(x_data))\n",
    "for i in range(len(x_data)):\n",
    "    output = model(x_data[i].unsqueeze(0))\n",
    "    torchvision.utils.save_image(output, 'E:/ANN_PROJECT/Predicted_Sketches/Sketch'+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_images_to_tensor(path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    files = os.listdir(path)\n",
    "    images = []\n",
    "    for i in files:\n",
    "        img_tensor = transform((Image.open(path + i)).convert('L'))\n",
    "        images.append(img_tensor)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Directory path containing the images\n",
    "directory = 'TestImage/'\n",
    "\n",
    "image_shapes = []\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    \n",
    "    # Open the image using PIL\n",
    "    image = Image.open(file_path)\n",
    "    \n",
    "    # Get the shape of the image\n",
    "    image_shapes.append(image.size)\n",
    "\n",
    "\n",
    "\n",
    "images = transform_test_images_to_tensor('TestImage/')\n",
    "images = torch.stack(images)\n",
    "print(images.shape)\n",
    "\n",
    "model = Autoencoder()  # Replace with your model architecture\n",
    "\n",
    "model_path = 'model.pth'\n",
    "model = torch.load(model_path , map_location=torch.device('cpu'))\n",
    "\n",
    "output = model(images)\n",
    "for i in range(len(output)):\n",
    "    # resized_image = F.interpolate(output[i].unsqueeze(0), size=image_shapes[i], mode='bilinear', align_corners=False)\n",
    "    torchvision.utils.save_image(output[i], 'TestSketch/Sketch'+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open the two images\n",
    "image1 = Image.open(\"TestImage/MadaraUchiha.jpg\")\n",
    "image2 = Image.open(\"TestSketch/Sketch2.png\")\n",
    "\n",
    "# Resize the images to have the same height\n",
    "height = min(image1.height, image2.height)\n",
    "image1 = image1.resize((int(image1.width * height / image1.height), height))\n",
    "image2 = image2.resize((int(image2.width * height / image2.height), height))\n",
    "\n",
    "# Create a new image with double the width\n",
    "new_width = image1.width + image2.width\n",
    "new_image = Image.new(\"RGB\", (new_width, height))\n",
    "\n",
    "# Paste the two images side by side\n",
    "new_image.paste(image1, (0, 0))\n",
    "new_image.paste(image2, (image1.width, 0))\n",
    "\n",
    "# Save the combined image\n",
    "new_image.save(\"combined_image4.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
